{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRARIES USED\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "import torch_pruning as tp\n",
    "import torchvision.models as models\n",
    "import caption\n",
    "from torchsummary import summary\n",
    "import torch.quantization as quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MACROS\n",
    "img = r\"C:\\Users\\xiaomi\\OneDrive\\TUM\\WS 2021-2022\\Advanced Topics in Communication Electronics\\image-captioning-on-pytorch-master\\image-captioning-on-pytorch-master\\img4.jpg\"\n",
    "word_map = r\"C:\\Users\\xiaomi\\OneDrive\\TUM\\WS 2021-2022\\Advanced Topics in Communication Electronics\\image-captioning-on-pytorch-master\\image-captioning-on-pytorch-master\\WORDMAP_coco_5_cap_per_img_5_min_word_freq.json\"\n",
    "model = r\"C:\\Users\\xiaomi\\OneDrive\\TUM\\WS 2021-2022\\Advanced Topics in Communication Electronics\\image-captioning-on-pytorch-master\\image-captioning-on-pytorch-master\\BEST_checkpoint_coco_5_cap_per_img_5_min_word_freq.pth.tar\"\n",
    "beam_size = 5\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE FUNCTION\n",
    "def inference(encoder, decoder, img, word_map = word_map, beam_size = beam_size):\n",
    "    seq, alphas = caption.caption_image_beam_search(encoder, decoder, img, word_map, beam_size)\n",
    "    alphas = torch.FloatTensor(alphas)\n",
    "    words = [rev_word_map[ind] for ind in seq]\n",
    "    sentence = \"\"\n",
    "    for word in words:\n",
    "        sentence = sentence + \" \" + word\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRUNE FUNCTION\n",
    "def prune_my_encoder(encoder):\n",
    "    for name, module in encoder.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            prune.ln_structured(module, name=\"weight\", amount=0.05, n=2, dim=0)\n",
    "            prune.remove(module, 'weight')\n",
    "        elif isinstance(module, torch.nn.Linear):\n",
    "            prune.ln_structured(module, name=\"weight\", amount=0.05, n=2, dim=0)\n",
    "            prune.remove(module, 'weight')\n",
    "    eps = 1e-3\n",
    "    with torch.no_grad():\n",
    "        for name, module in myencoder.named_modules():\n",
    "            if isinstance(module, torch.nn.Conv2d):\n",
    "                module.weight[abs(module.weight)<eps] = 0\n",
    "            elif isinstance(module, torch.nn.Linear):\n",
    "                module.weight[abs(module.weight)<eps] = 0            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODER CLASS\n",
    "class Encoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoded_image_size=14):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_image_size = encoded_image_size\n",
    "        self.quant = quantization.QuantStub()\n",
    "        self.dequant = quantization.DeQuantStub()\n",
    "        #resnet = torchvision.models.resnet101(pretrained=True)  # pretrained ImageNet ResNet-101\n",
    "        resnet = models.resnet101()\n",
    "        #resnet.load_state_dict(torch.load(\"resnet101-2.pth\"))\n",
    "\n",
    "        # Remove linear and pool layers (since we're not doing classification)\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = torch.nn.Sequential(*modules)\n",
    "\n",
    "        # Resize image to fixed size to allow input images of variable size\n",
    "        self.adaptive_pool = torch.nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
    "\n",
    "        self.fine_tune()\n",
    "        self.qconfig = quantization.get_default_qconfig('fbgemm')\n",
    "        # set the qengine to control weight packing\n",
    "        torch.backends.quantized.engine = 'fbgemm'\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        :param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n",
    "        :return: encoded images\n",
    "        \"\"\"\n",
    "#         images = quantization.QuantStub(images)\n",
    "        images = self.quant(images)\n",
    "        out = self.resnet(images)  # (batch_size, 2048, image_size/32, image_size/32)\n",
    "        out = self.adaptive_pool(out)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n",
    "        out = out.permute(0, 2, 3, 1)  # (batch_size, encoded_image_size, encoded_image_size, 2048)\n",
    "#         out = quantization.DeQuantStub(out)\n",
    "        out = self.dequant(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def fine_tune(self, fine_tune=True):\n",
    "        \"\"\"\n",
    "        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n",
    "\n",
    "        :param fine_tune: Allow?\n",
    "        \"\"\"\n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = False\n",
    "        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n",
    "        for c in list(self.resnet.children())[5:]:\n",
    "            for p in c.parameters():\n",
    "                p.requires_grad = fine_tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:671: SourceChangeWarning: source code of class 'models.Encoder' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.MaxPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:671: SourceChangeWarning: source code of class 'torchvision.models.resnet.Bottleneck' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.AdaptiveAvgPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:671: SourceChangeWarning: source code of class 'models.DecoderWithAttention' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:671: SourceChangeWarning: source code of class 'models.Attention' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.activation.Softmax' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.sparse.Embedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.rnn.LSTMCell' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.activation.Sigmoid' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(model, map_location=device)\n",
    "decoder = checkpoint['decoder']\n",
    "decoder = decoder.to(device)\n",
    "decoder.eval()\n",
    "encoder = checkpoint['encoder']\n",
    "encoder = encoder.to(device)\n",
    "encoder.eval()\n",
    "# Load word map (word2ix)\n",
    "with open(word_map, 'r') as j:\n",
    "    word_map = json.load(j)\n",
    "rev_word_map = {v: k for k, v in word_map.items()}  # ix2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       "  (resnet): Sequential(\n",
       "    (0): QuantStub()\n",
       "    (1): DeQuantStub()\n",
       "    (2): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (6): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (6): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (7): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (8): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (9): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (10): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (11): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (12): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (13): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (14): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (15): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (16): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (17): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (18): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (19): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (20): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (21): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (22): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (adaptive_pool): AdaptiveAvgPool2d(output_size=(14, 14))\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myencoder = Encoder()\n",
    "# myencoder.eval()\n",
    "myencoder = myencoder.to(device)\n",
    "myencoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = encoder.state_dict()\n",
    "sd2 = myencoder.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "for key in sd.keys():\n",
    "    sd2[key] = sd[key].type(torch.float16)\n",
    "    print(sd2[key].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet.2.weight torch.float32\n",
      "resnet.3.weight torch.float32\n",
      "resnet.3.bias torch.float32\n",
      "resnet.6.0.conv1.weight torch.float32\n",
      "resnet.6.0.bn1.weight torch.float32\n",
      "resnet.6.0.bn1.bias torch.float32\n",
      "resnet.6.0.conv2.weight torch.float32\n",
      "resnet.6.0.bn2.weight torch.float32\n",
      "resnet.6.0.bn2.bias torch.float32\n",
      "resnet.6.0.conv3.weight torch.float32\n",
      "resnet.6.0.bn3.weight torch.float32\n",
      "resnet.6.0.bn3.bias torch.float32\n",
      "resnet.6.0.downsample.0.weight torch.float32\n",
      "resnet.6.0.downsample.1.weight torch.float32\n",
      "resnet.6.0.downsample.1.bias torch.float32\n",
      "resnet.6.1.conv1.weight torch.float32\n",
      "resnet.6.1.bn1.weight torch.float32\n",
      "resnet.6.1.bn1.bias torch.float32\n",
      "resnet.6.1.conv2.weight torch.float32\n",
      "resnet.6.1.bn2.weight torch.float32\n",
      "resnet.6.1.bn2.bias torch.float32\n",
      "resnet.6.1.conv3.weight torch.float32\n",
      "resnet.6.1.bn3.weight torch.float32\n",
      "resnet.6.1.bn3.bias torch.float32\n",
      "resnet.6.2.conv1.weight torch.float32\n",
      "resnet.6.2.bn1.weight torch.float32\n",
      "resnet.6.2.bn1.bias torch.float32\n",
      "resnet.6.2.conv2.weight torch.float32\n",
      "resnet.6.2.bn2.weight torch.float32\n",
      "resnet.6.2.bn2.bias torch.float32\n",
      "resnet.6.2.conv3.weight torch.float32\n",
      "resnet.6.2.bn3.weight torch.float32\n",
      "resnet.6.2.bn3.bias torch.float32\n",
      "resnet.7.0.conv1.weight torch.float32\n",
      "resnet.7.0.bn1.weight torch.float32\n",
      "resnet.7.0.bn1.bias torch.float32\n",
      "resnet.7.0.conv2.weight torch.float32\n",
      "resnet.7.0.bn2.weight torch.float32\n",
      "resnet.7.0.bn2.bias torch.float32\n",
      "resnet.7.0.conv3.weight torch.float32\n",
      "resnet.7.0.bn3.weight torch.float32\n",
      "resnet.7.0.bn3.bias torch.float32\n",
      "resnet.7.0.downsample.0.weight torch.float32\n",
      "resnet.7.0.downsample.1.weight torch.float32\n",
      "resnet.7.0.downsample.1.bias torch.float32\n",
      "resnet.7.1.conv1.weight torch.float32\n",
      "resnet.7.1.bn1.weight torch.float32\n",
      "resnet.7.1.bn1.bias torch.float32\n",
      "resnet.7.1.conv2.weight torch.float32\n",
      "resnet.7.1.bn2.weight torch.float32\n",
      "resnet.7.1.bn2.bias torch.float32\n",
      "resnet.7.1.conv3.weight torch.float32\n",
      "resnet.7.1.bn3.weight torch.float32\n",
      "resnet.7.1.bn3.bias torch.float32\n",
      "resnet.7.2.conv1.weight torch.float32\n",
      "resnet.7.2.bn1.weight torch.float32\n",
      "resnet.7.2.bn1.bias torch.float32\n",
      "resnet.7.2.conv2.weight torch.float32\n",
      "resnet.7.2.bn2.weight torch.float32\n",
      "resnet.7.2.bn2.bias torch.float32\n",
      "resnet.7.2.conv3.weight torch.float32\n",
      "resnet.7.2.bn3.weight torch.float32\n",
      "resnet.7.2.bn3.bias torch.float32\n",
      "resnet.7.3.conv1.weight torch.float32\n",
      "resnet.7.3.bn1.weight torch.float32\n",
      "resnet.7.3.bn1.bias torch.float32\n",
      "resnet.7.3.conv2.weight torch.float32\n",
      "resnet.7.3.bn2.weight torch.float32\n",
      "resnet.7.3.bn2.bias torch.float32\n",
      "resnet.7.3.conv3.weight torch.float32\n",
      "resnet.7.3.bn3.weight torch.float32\n",
      "resnet.7.3.bn3.bias torch.float32\n",
      "resnet.8.0.conv1.weight torch.float32\n",
      "resnet.8.0.bn1.weight torch.float32\n",
      "resnet.8.0.bn1.bias torch.float32\n",
      "resnet.8.0.conv2.weight torch.float32\n",
      "resnet.8.0.bn2.weight torch.float32\n",
      "resnet.8.0.bn2.bias torch.float32\n",
      "resnet.8.0.conv3.weight torch.float32\n",
      "resnet.8.0.bn3.weight torch.float32\n",
      "resnet.8.0.bn3.bias torch.float32\n",
      "resnet.8.0.downsample.0.weight torch.float32\n",
      "resnet.8.0.downsample.1.weight torch.float32\n",
      "resnet.8.0.downsample.1.bias torch.float32\n",
      "resnet.8.1.conv1.weight torch.float32\n",
      "resnet.8.1.bn1.weight torch.float32\n",
      "resnet.8.1.bn1.bias torch.float32\n",
      "resnet.8.1.conv2.weight torch.float32\n",
      "resnet.8.1.bn2.weight torch.float32\n",
      "resnet.8.1.bn2.bias torch.float32\n",
      "resnet.8.1.conv3.weight torch.float32\n",
      "resnet.8.1.bn3.weight torch.float32\n",
      "resnet.8.1.bn3.bias torch.float32\n",
      "resnet.8.2.conv1.weight torch.float32\n",
      "resnet.8.2.bn1.weight torch.float32\n",
      "resnet.8.2.bn1.bias torch.float32\n",
      "resnet.8.2.conv2.weight torch.float32\n",
      "resnet.8.2.bn2.weight torch.float32\n",
      "resnet.8.2.bn2.bias torch.float32\n",
      "resnet.8.2.conv3.weight torch.float32\n",
      "resnet.8.2.bn3.weight torch.float32\n",
      "resnet.8.2.bn3.bias torch.float32\n",
      "resnet.8.3.conv1.weight torch.float32\n",
      "resnet.8.3.bn1.weight torch.float32\n",
      "resnet.8.3.bn1.bias torch.float32\n",
      "resnet.8.3.conv2.weight torch.float32\n",
      "resnet.8.3.bn2.weight torch.float32\n",
      "resnet.8.3.bn2.bias torch.float32\n",
      "resnet.8.3.conv3.weight torch.float32\n",
      "resnet.8.3.bn3.weight torch.float32\n",
      "resnet.8.3.bn3.bias torch.float32\n",
      "resnet.8.4.conv1.weight torch.float32\n",
      "resnet.8.4.bn1.weight torch.float32\n",
      "resnet.8.4.bn1.bias torch.float32\n",
      "resnet.8.4.conv2.weight torch.float32\n",
      "resnet.8.4.bn2.weight torch.float32\n",
      "resnet.8.4.bn2.bias torch.float32\n",
      "resnet.8.4.conv3.weight torch.float32\n",
      "resnet.8.4.bn3.weight torch.float32\n",
      "resnet.8.4.bn3.bias torch.float32\n",
      "resnet.8.5.conv1.weight torch.float32\n",
      "resnet.8.5.bn1.weight torch.float32\n",
      "resnet.8.5.bn1.bias torch.float32\n",
      "resnet.8.5.conv2.weight torch.float32\n",
      "resnet.8.5.bn2.weight torch.float32\n",
      "resnet.8.5.bn2.bias torch.float32\n",
      "resnet.8.5.conv3.weight torch.float32\n",
      "resnet.8.5.bn3.weight torch.float32\n",
      "resnet.8.5.bn3.bias torch.float32\n",
      "resnet.8.6.conv1.weight torch.float32\n",
      "resnet.8.6.bn1.weight torch.float32\n",
      "resnet.8.6.bn1.bias torch.float32\n",
      "resnet.8.6.conv2.weight torch.float32\n",
      "resnet.8.6.bn2.weight torch.float32\n",
      "resnet.8.6.bn2.bias torch.float32\n",
      "resnet.8.6.conv3.weight torch.float32\n",
      "resnet.8.6.bn3.weight torch.float32\n",
      "resnet.8.6.bn3.bias torch.float32\n",
      "resnet.8.7.conv1.weight torch.float32\n",
      "resnet.8.7.bn1.weight torch.float32\n",
      "resnet.8.7.bn1.bias torch.float32\n",
      "resnet.8.7.conv2.weight torch.float32\n",
      "resnet.8.7.bn2.weight torch.float32\n",
      "resnet.8.7.bn2.bias torch.float32\n",
      "resnet.8.7.conv3.weight torch.float32\n",
      "resnet.8.7.bn3.weight torch.float32\n",
      "resnet.8.7.bn3.bias torch.float32\n",
      "resnet.8.8.conv1.weight torch.float32\n",
      "resnet.8.8.bn1.weight torch.float32\n",
      "resnet.8.8.bn1.bias torch.float32\n",
      "resnet.8.8.conv2.weight torch.float32\n",
      "resnet.8.8.bn2.weight torch.float32\n",
      "resnet.8.8.bn2.bias torch.float32\n",
      "resnet.8.8.conv3.weight torch.float32\n",
      "resnet.8.8.bn3.weight torch.float32\n",
      "resnet.8.8.bn3.bias torch.float32\n",
      "resnet.8.9.conv1.weight torch.float32\n",
      "resnet.8.9.bn1.weight torch.float32\n",
      "resnet.8.9.bn1.bias torch.float32\n",
      "resnet.8.9.conv2.weight torch.float32\n",
      "resnet.8.9.bn2.weight torch.float32\n",
      "resnet.8.9.bn2.bias torch.float32\n",
      "resnet.8.9.conv3.weight torch.float32\n",
      "resnet.8.9.bn3.weight torch.float32\n",
      "resnet.8.9.bn3.bias torch.float32\n",
      "resnet.8.10.conv1.weight torch.float32\n",
      "resnet.8.10.bn1.weight torch.float32\n",
      "resnet.8.10.bn1.bias torch.float32\n",
      "resnet.8.10.conv2.weight torch.float32\n",
      "resnet.8.10.bn2.weight torch.float32\n",
      "resnet.8.10.bn2.bias torch.float32\n",
      "resnet.8.10.conv3.weight torch.float32\n",
      "resnet.8.10.bn3.weight torch.float32\n",
      "resnet.8.10.bn3.bias torch.float32\n",
      "resnet.8.11.conv1.weight torch.float32\n",
      "resnet.8.11.bn1.weight torch.float32\n",
      "resnet.8.11.bn1.bias torch.float32\n",
      "resnet.8.11.conv2.weight torch.float32\n",
      "resnet.8.11.bn2.weight torch.float32\n",
      "resnet.8.11.bn2.bias torch.float32\n",
      "resnet.8.11.conv3.weight torch.float32\n",
      "resnet.8.11.bn3.weight torch.float32\n",
      "resnet.8.11.bn3.bias torch.float32\n",
      "resnet.8.12.conv1.weight torch.float32\n",
      "resnet.8.12.bn1.weight torch.float32\n",
      "resnet.8.12.bn1.bias torch.float32\n",
      "resnet.8.12.conv2.weight torch.float32\n",
      "resnet.8.12.bn2.weight torch.float32\n",
      "resnet.8.12.bn2.bias torch.float32\n",
      "resnet.8.12.conv3.weight torch.float32\n",
      "resnet.8.12.bn3.weight torch.float32\n",
      "resnet.8.12.bn3.bias torch.float32\n",
      "resnet.8.13.conv1.weight torch.float32\n",
      "resnet.8.13.bn1.weight torch.float32\n",
      "resnet.8.13.bn1.bias torch.float32\n",
      "resnet.8.13.conv2.weight torch.float32\n",
      "resnet.8.13.bn2.weight torch.float32\n",
      "resnet.8.13.bn2.bias torch.float32\n",
      "resnet.8.13.conv3.weight torch.float32\n",
      "resnet.8.13.bn3.weight torch.float32\n",
      "resnet.8.13.bn3.bias torch.float32\n",
      "resnet.8.14.conv1.weight torch.float32\n",
      "resnet.8.14.bn1.weight torch.float32\n",
      "resnet.8.14.bn1.bias torch.float32\n",
      "resnet.8.14.conv2.weight torch.float32\n",
      "resnet.8.14.bn2.weight torch.float32\n",
      "resnet.8.14.bn2.bias torch.float32\n",
      "resnet.8.14.conv3.weight torch.float32\n",
      "resnet.8.14.bn3.weight torch.float32\n",
      "resnet.8.14.bn3.bias torch.float32\n",
      "resnet.8.15.conv1.weight torch.float32\n",
      "resnet.8.15.bn1.weight torch.float32\n",
      "resnet.8.15.bn1.bias torch.float32\n",
      "resnet.8.15.conv2.weight torch.float32\n",
      "resnet.8.15.bn2.weight torch.float32\n",
      "resnet.8.15.bn2.bias torch.float32\n",
      "resnet.8.15.conv3.weight torch.float32\n",
      "resnet.8.15.bn3.weight torch.float32\n",
      "resnet.8.15.bn3.bias torch.float32\n",
      "resnet.8.16.conv1.weight torch.float32\n",
      "resnet.8.16.bn1.weight torch.float32\n",
      "resnet.8.16.bn1.bias torch.float32\n",
      "resnet.8.16.conv2.weight torch.float32\n",
      "resnet.8.16.bn2.weight torch.float32\n",
      "resnet.8.16.bn2.bias torch.float32\n",
      "resnet.8.16.conv3.weight torch.float32\n",
      "resnet.8.16.bn3.weight torch.float32\n",
      "resnet.8.16.bn3.bias torch.float32\n",
      "resnet.8.17.conv1.weight torch.float32\n",
      "resnet.8.17.bn1.weight torch.float32\n",
      "resnet.8.17.bn1.bias torch.float32\n",
      "resnet.8.17.conv2.weight torch.float32\n",
      "resnet.8.17.bn2.weight torch.float32\n",
      "resnet.8.17.bn2.bias torch.float32\n",
      "resnet.8.17.conv3.weight torch.float32\n",
      "resnet.8.17.bn3.weight torch.float32\n",
      "resnet.8.17.bn3.bias torch.float32\n",
      "resnet.8.18.conv1.weight torch.float32\n",
      "resnet.8.18.bn1.weight torch.float32\n",
      "resnet.8.18.bn1.bias torch.float32\n",
      "resnet.8.18.conv2.weight torch.float32\n",
      "resnet.8.18.bn2.weight torch.float32\n",
      "resnet.8.18.bn2.bias torch.float32\n",
      "resnet.8.18.conv3.weight torch.float32\n",
      "resnet.8.18.bn3.weight torch.float32\n",
      "resnet.8.18.bn3.bias torch.float32\n",
      "resnet.8.19.conv1.weight torch.float32\n",
      "resnet.8.19.bn1.weight torch.float32\n",
      "resnet.8.19.bn1.bias torch.float32\n",
      "resnet.8.19.conv2.weight torch.float32\n",
      "resnet.8.19.bn2.weight torch.float32\n",
      "resnet.8.19.bn2.bias torch.float32\n",
      "resnet.8.19.conv3.weight torch.float32\n",
      "resnet.8.19.bn3.weight torch.float32\n",
      "resnet.8.19.bn3.bias torch.float32\n",
      "resnet.8.20.conv1.weight torch.float32\n",
      "resnet.8.20.bn1.weight torch.float32\n",
      "resnet.8.20.bn1.bias torch.float32\n",
      "resnet.8.20.conv2.weight torch.float32\n",
      "resnet.8.20.bn2.weight torch.float32\n",
      "resnet.8.20.bn2.bias torch.float32\n",
      "resnet.8.20.conv3.weight torch.float32\n",
      "resnet.8.20.bn3.weight torch.float32\n",
      "resnet.8.20.bn3.bias torch.float32\n",
      "resnet.8.21.conv1.weight torch.float32\n",
      "resnet.8.21.bn1.weight torch.float32\n",
      "resnet.8.21.bn1.bias torch.float32\n",
      "resnet.8.21.conv2.weight torch.float32\n",
      "resnet.8.21.bn2.weight torch.float32\n",
      "resnet.8.21.bn2.bias torch.float32\n",
      "resnet.8.21.conv3.weight torch.float32\n",
      "resnet.8.21.bn3.weight torch.float32\n",
      "resnet.8.21.bn3.bias torch.float32\n",
      "resnet.8.22.conv1.weight torch.float32\n",
      "resnet.8.22.bn1.weight torch.float32\n",
      "resnet.8.22.bn1.bias torch.float32\n",
      "resnet.8.22.conv2.weight torch.float32\n",
      "resnet.8.22.bn2.weight torch.float32\n",
      "resnet.8.22.bn2.bias torch.float32\n",
      "resnet.8.22.conv3.weight torch.float32\n",
      "resnet.8.22.bn3.weight torch.float32\n",
      "resnet.8.22.bn3.bias torch.float32\n",
      "resnet.9.0.conv1.weight torch.float32\n",
      "resnet.9.0.bn1.weight torch.float32\n",
      "resnet.9.0.bn1.bias torch.float32\n",
      "resnet.9.0.conv2.weight torch.float32\n",
      "resnet.9.0.bn2.weight torch.float32\n",
      "resnet.9.0.bn2.bias torch.float32\n",
      "resnet.9.0.conv3.weight torch.float32\n",
      "resnet.9.0.bn3.weight torch.float32\n",
      "resnet.9.0.bn3.bias torch.float32\n",
      "resnet.9.0.downsample.0.weight torch.float32\n",
      "resnet.9.0.downsample.1.weight torch.float32\n",
      "resnet.9.0.downsample.1.bias torch.float32\n",
      "resnet.9.1.conv1.weight torch.float32\n",
      "resnet.9.1.bn1.weight torch.float32\n",
      "resnet.9.1.bn1.bias torch.float32\n",
      "resnet.9.1.conv2.weight torch.float32\n",
      "resnet.9.1.bn2.weight torch.float32\n",
      "resnet.9.1.bn2.bias torch.float32\n",
      "resnet.9.1.conv3.weight torch.float32\n",
      "resnet.9.1.bn3.weight torch.float32\n",
      "resnet.9.1.bn3.bias torch.float32\n",
      "resnet.9.2.conv1.weight torch.float32\n",
      "resnet.9.2.bn1.weight torch.float32\n",
      "resnet.9.2.bn1.bias torch.float32\n",
      "resnet.9.2.conv2.weight torch.float32\n",
      "resnet.9.2.bn2.weight torch.float32\n",
      "resnet.9.2.bn2.bias torch.float32\n",
      "resnet.9.2.conv3.weight torch.float32\n",
      "resnet.9.2.bn3.weight torch.float32\n",
      "resnet.9.2.bn3.bias torch.float32\n"
     ]
    }
   ],
   "source": [
    "for name, param in myencoder.named_parameters():\n",
    "    print(name,param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Encoder:\n\tUnexpected key(s) in state_dict: \"resnet.0.weight\", \"resnet.1.weight\", \"resnet.1.bias\", \"resnet.1.running_mean\", \"resnet.1.running_var\", \"resnet.4.0.conv1.weight\", \"resnet.4.0.bn1.weight\", \"resnet.4.0.bn1.bias\", \"resnet.4.0.bn1.running_mean\", \"resnet.4.0.bn1.running_var\", \"resnet.4.0.conv2.weight\", \"resnet.4.0.bn2.weight\", \"resnet.4.0.bn2.bias\", \"resnet.4.0.bn2.running_mean\", \"resnet.4.0.bn2.running_var\", \"resnet.4.0.conv3.weight\", \"resnet.4.0.bn3.weight\", \"resnet.4.0.bn3.bias\", \"resnet.4.0.bn3.running_mean\", \"resnet.4.0.bn3.running_var\", \"resnet.4.0.downsample.0.weight\", \"resnet.4.0.downsample.1.weight\", \"resnet.4.0.downsample.1.bias\", \"resnet.4.0.downsample.1.running_mean\", \"resnet.4.0.downsample.1.running_var\", \"resnet.4.1.conv1.weight\", \"resnet.4.1.bn1.weight\", \"resnet.4.1.bn1.bias\", \"resnet.4.1.bn1.running_mean\", \"resnet.4.1.bn1.running_var\", \"resnet.4.1.conv2.weight\", \"resnet.4.1.bn2.weight\", \"resnet.4.1.bn2.bias\", \"resnet.4.1.bn2.running_mean\", \"resnet.4.1.bn2.running_var\", \"resnet.4.1.conv3.weight\", \"resnet.4.1.bn3.weight\", \"resnet.4.1.bn3.bias\", \"resnet.4.1.bn3.running_mean\", \"resnet.4.1.bn3.running_var\", \"resnet.4.2.conv1.weight\", \"resnet.4.2.bn1.weight\", \"resnet.4.2.bn1.bias\", \"resnet.4.2.bn1.running_mean\", \"resnet.4.2.bn1.running_var\", \"resnet.4.2.conv2.weight\", \"resnet.4.2.bn2.weight\", \"resnet.4.2.bn2.bias\", \"resnet.4.2.bn2.running_mean\", \"resnet.4.2.bn2.running_var\", \"resnet.4.2.conv3.weight\", \"resnet.4.2.bn3.weight\", \"resnet.4.2.bn3.bias\", \"resnet.4.2.bn3.running_mean\", \"resnet.4.2.bn3.running_var\", \"resnet.5.0.conv1.weight\", \"resnet.5.0.bn1.weight\", \"resnet.5.0.bn1.bias\", \"resnet.5.0.bn1.running_mean\", \"resnet.5.0.bn1.running_var\", \"resnet.5.0.conv2.weight\", \"resnet.5.0.bn2.weight\", \"resnet.5.0.bn2.bias\", \"resnet.5.0.bn2.running_mean\", \"resnet.5.0.bn2.running_var\", \"resnet.5.0.conv3.weight\", \"resnet.5.0.bn3.weight\", \"resnet.5.0.bn3.bias\", \"resnet.5.0.bn3.running_mean\", \"resnet.5.0.bn3.running_var\", \"resnet.5.0.downsample.0.weight\", \"resnet.5.0.downsample.1.weight\", \"resnet.5.0.downsample.1.bias\", \"resnet.5.0.downsample.1.running_mean\", \"resnet.5.0.downsample.1.running_var\", \"resnet.5.1.conv1.weight\", \"resnet.5.1.bn1.weight\", \"resnet.5.1.bn1.bias\", \"resnet.5.1.bn1.running_mean\", \"resnet.5.1.bn1.running_var\", \"resnet.5.1.conv2.weight\", \"resnet.5.1.bn2.weight\", \"resnet.5.1.bn2.bias\", \"resnet.5.1.bn2.running_mean\", \"resnet.5.1.bn2.running_var\", \"resnet.5.1.conv3.weight\", \"resnet.5.1.bn3.weight\", \"resnet.5.1.bn3.bias\", \"resnet.5.1.bn3.running_mean\", \"resnet.5.1.bn3.running_var\", \"resnet.5.2.conv1.weight\", \"resnet.5.2.bn1.weight\", \"resnet.5.2.bn1.bias\", \"resnet.5.2.bn1.running_mean\", \"resnet.5.2.bn1.running_var\", \"resnet.5.2.conv2.weight\", \"resnet.5.2.bn2.weight\", \"resnet.5.2.bn2.bias\", \"resnet.5.2.bn2.running_mean\", \"resnet.5.2.bn2.running_var\", \"resnet.5.2.conv3.weight\", \"resnet.5.2.bn3.weight\", \"resnet.5.2.bn3.bias\", \"resnet.5.2.bn3.running_mean\", \"resnet.5.2.bn3.running_var\", \"resnet.5.3.conv1.weight\", \"resnet.5.3.bn1.weight\", \"resnet.5.3.bn1.bias\", \"resnet.5.3.bn1.running_mean\", \"resnet.5.3.bn1.running_var\", \"resnet.5.3.conv2.weight\", \"resnet.5.3.bn2.weight\", \"resnet.5.3.bn2.bias\", \"resnet.5.3.bn2.running_mean\", \"resnet.5.3.bn2.running_var\", \"resnet.5.3.conv3.weight\", \"resnet.5.3.bn3.weight\", \"resnet.5.3.bn3.bias\", \"resnet.5.3.bn3.running_mean\", \"resnet.5.3.bn3.running_var\", \"resnet.6.3.conv1.weight\", \"resnet.6.3.bn1.weight\", \"resnet.6.3.bn1.bias\", \"resnet.6.3.bn1.running_mean\", \"resnet.6.3.bn1.running_var\", \"resnet.6.3.conv2.weight\", \"resnet.6.3.bn2.weight\", \"resnet.6.3.bn2.bias\", \"resnet.6.3.bn2.running_mean\", \"resnet.6.3.bn2.running_var\", \"resnet.6.3.conv3.weight\", \"resnet.6.3.bn3.weight\", \"resnet.6.3.bn3.bias\", \"resnet.6.3.bn3.running_mean\", \"resnet.6.3.bn3.running_var\", \"resnet.6.4.conv1.weight\", \"resnet.6.4.bn1.weight\", \"resnet.6.4.bn1.bias\", \"resnet.6.4.bn1.running_mean\", \"resnet.6.4.bn1.running_var\", \"resnet.6.4.conv2.weight\", \"resnet.6.4.bn2.weight\", \"resnet.6.4.bn2.bias\", \"resnet.6.4.bn2.running_mean\", \"resnet.6.4.bn2.running_var\", \"resnet.6.4.conv3.weight\", \"resnet.6.4.bn3.weight\", \"resnet.6.4.bn3.bias\", \"resnet.6.4.bn3.running_mean\", \"resnet.6.4.bn3.running_var\", \"resnet.6.5.conv1.weight\", \"resnet.6.5.bn1.weight\", \"resnet.6.5.bn1.bias\", \"resnet.6.5.bn1.running_mean\", \"resnet.6.5.bn1.running_var\", \"resnet.6.5.conv2.weight\", \"resnet.6.5.bn2.weight\", \"resnet.6.5.bn2.bias\", \"resnet.6.5.bn2.running_mean\", \"resnet.6.5.bn2.running_var\", \"resnet.6.5.conv3.weight\", \"resnet.6.5.bn3.weight\", \"resnet.6.5.bn3.bias\", \"resnet.6.5.bn3.running_mean\", \"resnet.6.5.bn3.running_var\", \"resnet.6.6.conv1.weight\", \"resnet.6.6.bn1.weight\", \"resnet.6.6.bn1.bias\", \"resnet.6.6.bn1.running_mean\", \"resnet.6.6.bn1.running_var\", \"resnet.6.6.conv2.weight\", \"resnet.6.6.bn2.weight\", \"resnet.6.6.bn2.bias\", \"resnet.6.6.bn2.running_mean\", \"resnet.6.6.bn2.running_var\", \"resnet.6.6.conv3.weight\", \"resnet.6.6.bn3.weight\", \"resnet.6.6.bn3.bias\", \"resnet.6.6.bn3.running_mean\", \"resnet.6.6.bn3.running_var\", \"resnet.6.7.conv1.weight\", \"resnet.6.7.bn1.weight\", \"resnet.6.7.bn1.bias\", \"resnet.6.7.bn1.running_mean\", \"resnet.6.7.bn1.running_var\", \"resnet.6.7.conv2.weight\", \"resnet.6.7.bn2.weight\", \"resnet.6.7.bn2.bias\", \"resnet.6.7.bn2.running_mean\", \"resnet.6.7.bn2.running_var\", \"resnet.6.7.conv3.weight\", \"resnet.6.7.bn3.weight\", \"resnet.6.7.bn3.bias\", \"resnet.6.7.bn3.running_mean\", \"resnet.6.7.bn3.running_var\", \"resnet.6.8.conv1.weight\", \"resnet.6.8.bn1.weight\", \"resnet.6.8.bn1.bias\", \"resnet.6.8.bn1.running_mean\", \"resnet.6.8.bn1.running_var\", \"resnet.6.8.conv2.weight\", \"resnet.6.8.bn2.weight\", \"resnet.6.8.bn2.bias\", \"resnet.6.8.bn2.running_mean\", \"resnet.6.8.bn2.running_var\", \"resnet.6.8.conv3.weight\", \"resnet.6.8.bn3.weight\", \"resnet.6.8.bn3.bias\", \"resnet.6.8.bn3.running_mean\", \"resnet.6.8.bn3.running_var\", \"resnet.6.9.conv1.weight\", \"resnet.6.9.bn1.weight\", \"resnet.6.9.bn1.bias\", \"resnet.6.9.bn1.running_mean\", \"resnet.6.9.bn1.running_var\", \"resnet.6.9.conv2.weight\", \"resnet.6.9.bn2.weight\", \"resnet.6.9.bn2.bias\", \"resnet.6.9.bn2.running_mean\", \"resnet.6.9.bn2.running_var\", \"resnet.6.9.conv3.weight\", \"resnet.6.9.bn3.weight\", \"resnet.6.9.bn3.bias\", \"resnet.6.9.bn3.running_mean\", \"resnet.6.9.bn3.running_var\", \"resnet.6.10.conv1.weight\", \"resnet.6.10.bn1.weight\", \"resnet.6.10.bn1.bias\", \"resnet.6.10.bn1.running_mean\", \"resnet.6.10.bn1.running_var\", \"resnet.6.10.conv2.weight\", \"resnet.6.10.bn2.weight\", \"resnet.6.10.bn2.bias\", \"resnet.6.10.bn2.running_mean\", \"resnet.6.10.bn2.running_var\", \"resnet.6.10.conv3.weight\", \"resnet.6.10.bn3.weight\", \"resnet.6.10.bn3.bias\", \"resnet.6.10.bn3.running_mean\", \"resnet.6.10.bn3.running_var\", \"resnet.6.11.conv1.weight\", \"resnet.6.11.bn1.weight\", \"resnet.6.11.bn1.bias\", \"resnet.6.11.bn1.running_mean\", \"resnet.6.11.bn1.running_var\", \"resnet.6.11.conv2.weight\", \"resnet.6.11.bn2.weight\", \"resnet.6.11.bn2.bias\", \"resnet.6.11.bn2.running_mean\", \"resnet.6.11.bn2.running_var\", \"resnet.6.11.conv3.weight\", \"resnet.6.11.bn3.weight\", \"resnet.6.11.bn3.bias\", \"resnet.6.11.bn3.running_mean\", \"resnet.6.11.bn3.running_var\", \"resnet.6.12.conv1.weight\", \"resnet.6.12.bn1.weight\", \"resnet.6.12.bn1.bias\", \"resnet.6.12.bn1.running_mean\", \"resnet.6.12.bn1.running_var\", \"resnet.6.12.conv2.weight\", \"resnet.6.12.bn2.weight\", \"resnet.6.12.bn2.bias\", \"resnet.6.12.bn2.running_mean\", \"resnet.6.12.bn2.running_var\", \"resnet.6.12.conv3.weight\", \"resnet.6.12.bn3.weight\", \"resnet.6.12.bn3.bias\", \"resnet.6.12.bn3.running_mean\", \"resnet.6.12.bn3.running_var\", \"resnet.6.13.conv1.weight\", \"resnet.6.13.bn1.weight\", \"resnet.6.13.bn1.bias\", \"resnet.6.13.bn1.running_mean\", \"resnet.6.13.bn1.running_var\", \"resnet.6.13.conv2.weight\", \"resnet.6.13.bn2.weight\", \"resnet.6.13.bn2.bias\", \"resnet.6.13.bn2.running_mean\", \"resnet.6.13.bn2.running_var\", \"resnet.6.13.conv3.weight\", \"resnet.6.13.bn3.weight\", \"resnet.6.13.bn3.bias\", \"resnet.6.13.bn3.running_mean\", \"resnet.6.13.bn3.running_var\", \"resnet.6.14.conv1.weight\", \"resnet.6.14.bn1.weight\", \"resnet.6.14.bn1.bias\", \"resnet.6.14.bn1.running_mean\", \"resnet.6.14.bn1.running_var\", \"resnet.6.14.conv2.weight\", \"resnet.6.14.bn2.weight\", \"resnet.6.14.bn2.bias\", \"resnet.6.14.bn2.running_mean\", \"resnet.6.14.bn2.running_var\", \"resnet.6.14.conv3.weight\", \"resnet.6.14.bn3.weight\", \"resnet.6.14.bn3.bias\", \"resnet.6.14.bn3.running_mean\", \"resnet.6.14.bn3.running_var\", \"resnet.6.15.conv1.weight\", \"resnet.6.15.bn1.weight\", \"resnet.6.15.bn1.bias\", \"resnet.6.15.bn1.running_mean\", \"resnet.6.15.bn1.running_var\", \"resnet.6.15.conv2.weight\", \"resnet.6.15.bn2.weight\", \"resnet.6.15.bn2.bias\", \"resnet.6.15.bn2.running_mean\", \"resnet.6.15.bn2.running_var\", \"resnet.6.15.conv3.weight\", \"resnet.6.15.bn3.weight\", \"resnet.6.15.bn3.bias\", \"resnet.6.15.bn3.running_mean\", \"resnet.6.15.bn3.running_var\", \"resnet.6.16.conv1.weight\", \"resnet.6.16.bn1.weight\", \"resnet.6.16.bn1.bias\", \"resnet.6.16.bn1.running_mean\", \"resnet.6.16.bn1.running_var\", \"resnet.6.16.conv2.weight\", \"resnet.6.16.bn2.weight\", \"resnet.6.16.bn2.bias\", \"resnet.6.16.bn2.running_mean\", \"resnet.6.16.bn2.running_var\", \"resnet.6.16.conv3.weight\", \"resnet.6.16.bn3.weight\", \"resnet.6.16.bn3.bias\", \"resnet.6.16.bn3.running_mean\", \"resnet.6.16.bn3.running_var\", \"resnet.6.17.conv1.weight\", \"resnet.6.17.bn1.weight\", \"resnet.6.17.bn1.bias\", \"resnet.6.17.bn1.running_mean\", \"resnet.6.17.bn1.running_var\", \"resnet.6.17.conv2.weight\", \"resnet.6.17.bn2.weight\", \"resnet.6.17.bn2.bias\", \"resnet.6.17.bn2.running_mean\", \"resnet.6.17.bn2.running_var\", \"resnet.6.17.conv3.weight\", \"resnet.6.17.bn3.weight\", \"resnet.6.17.bn3.bias\", \"resnet.6.17.bn3.running_mean\", \"resnet.6.17.bn3.running_var\", \"resnet.6.18.conv1.weight\", \"resnet.6.18.bn1.weight\", \"resnet.6.18.bn1.bias\", \"resnet.6.18.bn1.running_mean\", \"resnet.6.18.bn1.running_var\", \"resnet.6.18.conv2.weight\", \"resnet.6.18.bn2.weight\", \"resnet.6.18.bn2.bias\", \"resnet.6.18.bn2.running_mean\", \"resnet.6.18.bn2.running_var\", \"resnet.6.18.conv3.weight\", \"resnet.6.18.bn3.weight\", \"resnet.6.18.bn3.bias\", \"resnet.6.18.bn3.running_mean\", \"resnet.6.18.bn3.running_var\", \"resnet.6.19.conv1.weight\", \"resnet.6.19.bn1.weight\", \"resnet.6.19.bn1.bias\", \"resnet.6.19.bn1.running_mean\", \"resnet.6.19.bn1.running_var\", \"resnet.6.19.conv2.weight\", \"resnet.6.19.bn2.weight\", \"resnet.6.19.bn2.bias\", \"resnet.6.19.bn2.running_mean\", \"resnet.6.19.bn2.running_var\", \"resnet.6.19.conv3.weight\", \"resnet.6.19.bn3.weight\", \"resnet.6.19.bn3.bias\", \"resnet.6.19.bn3.running_mean\", \"resnet.6.19.bn3.running_var\", \"resnet.6.20.conv1.weight\", \"resnet.6.20.bn1.weight\", \"resnet.6.20.bn1.bias\", \"resnet.6.20.bn1.running_mean\", \"resnet.6.20.bn1.running_var\", \"resnet.6.20.conv2.weight\", \"resnet.6.20.bn2.weight\", \"resnet.6.20.bn2.bias\", \"resnet.6.20.bn2.running_mean\", \"resnet.6.20.bn2.running_var\", \"resnet.6.20.conv3.weight\", \"resnet.6.20.bn3.weight\", \"resnet.6.20.bn3.bias\", \"resnet.6.20.bn3.running_mean\", \"resnet.6.20.bn3.running_var\", \"resnet.6.21.conv1.weight\", \"resnet.6.21.bn1.weight\", \"resnet.6.21.bn1.bias\", \"resnet.6.21.bn1.running_mean\", \"resnet.6.21.bn1.running_var\", \"resnet.6.21.conv2.weight\", \"resnet.6.21.bn2.weight\", \"resnet.6.21.bn2.bias\", \"resnet.6.21.bn2.running_mean\", \"resnet.6.21.bn2.running_var\", \"resnet.6.21.conv3.weight\", \"resnet.6.21.bn3.weight\", \"resnet.6.21.bn3.bias\", \"resnet.6.21.bn3.running_mean\", \"resnet.6.21.bn3.running_var\", \"resnet.6.22.conv1.weight\", \"resnet.6.22.bn1.weight\", \"resnet.6.22.bn1.bias\", \"resnet.6.22.bn1.running_mean\", \"resnet.6.22.bn1.running_var\", \"resnet.6.22.conv2.weight\", \"resnet.6.22.bn2.weight\", \"resnet.6.22.bn2.bias\", \"resnet.6.22.bn2.running_mean\", \"resnet.6.22.bn2.running_var\", \"resnet.6.22.conv3.weight\", \"resnet.6.22.bn3.weight\", \"resnet.6.22.bn3.bias\", \"resnet.6.22.bn3.running_mean\", \"resnet.6.22.bn3.running_var\". \n\tsize mismatch for resnet.6.0.conv1.weight: copying a param with shape torch.Size([256, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n\tsize mismatch for resnet.6.0.bn1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.0.bn1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.0.bn1.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.0.bn1.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.0.conv2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for resnet.6.0.bn2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.0.bn2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.0.bn2.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.0.bn2.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.0.conv3.weight: copying a param with shape torch.Size([1024, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 64, 1, 1]).\n\tsize mismatch for resnet.6.0.bn3.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.6.0.bn3.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.6.0.bn3.running_mean: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.6.0.bn3.running_var: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.6.0.downsample.0.weight: copying a param with shape torch.Size([1024, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 64, 1, 1]).\n\tsize mismatch for resnet.6.0.downsample.1.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.6.0.downsample.1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.6.0.downsample.1.running_mean: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.6.0.downsample.1.running_var: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.6.1.conv1.weight: copying a param with shape torch.Size([256, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n\tsize mismatch for resnet.6.1.bn1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.1.bn1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.1.bn1.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.1.bn1.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.1.conv2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for resnet.6.1.bn2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.1.bn2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.1.bn2.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.1.bn2.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.1.conv3.weight: copying a param with shape torch.Size([1024, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 64, 1, 1]).\n\tsize mismatch for resnet.6.1.bn3.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.6.1.bn3.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.6.1.bn3.running_mean: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.6.1.bn3.running_var: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.6.2.conv1.weight: copying a param with shape torch.Size([256, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n\tsize mismatch for resnet.6.2.bn1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.2.bn1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.2.bn1.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.2.bn1.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.2.conv2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for resnet.6.2.bn2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.2.bn2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.2.bn2.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.2.bn2.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.2.conv3.weight: copying a param with shape torch.Size([1024, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 64, 1, 1]).\n\tsize mismatch for resnet.6.2.bn3.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.6.2.bn3.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.6.2.bn3.running_mean: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.6.2.bn3.running_var: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.7.0.conv1.weight: copying a param with shape torch.Size([512, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for resnet.7.0.bn1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.0.bn1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.0.bn1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.0.bn1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.0.conv2.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for resnet.7.0.bn2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.0.bn2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.0.bn2.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.0.bn2.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.0.conv3.weight: copying a param with shape torch.Size([2048, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 128, 1, 1]).\n\tsize mismatch for resnet.7.0.bn3.weight: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.7.0.bn3.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.7.0.bn3.running_mean: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.7.0.bn3.running_var: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.7.0.downsample.0.weight: copying a param with shape torch.Size([2048, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for resnet.7.0.downsample.1.weight: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.7.0.downsample.1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.7.0.downsample.1.running_mean: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.7.0.downsample.1.running_var: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.7.1.conv1.weight: copying a param with shape torch.Size([512, 2048, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for resnet.7.1.bn1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.1.bn1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.1.bn1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.1.bn1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.1.conv2.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for resnet.7.1.bn2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.1.bn2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.1.bn2.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.1.bn2.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.1.conv3.weight: copying a param with shape torch.Size([2048, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 128, 1, 1]).\n\tsize mismatch for resnet.7.1.bn3.weight: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.7.1.bn3.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.7.1.bn3.running_mean: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.7.1.bn3.running_var: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.7.2.conv1.weight: copying a param with shape torch.Size([512, 2048, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for resnet.7.2.bn1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.2.bn1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.2.bn1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.2.bn1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.2.conv2.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for resnet.7.2.bn2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.2.bn2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.2.bn2.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.2.bn2.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.2.conv3.weight: copying a param with shape torch.Size([2048, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 128, 1, 1]).\n\tsize mismatch for resnet.7.2.bn3.weight: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.7.2.bn3.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.7.2.bn3.running_mean: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.7.2.bn3.running_var: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17324/1454665718.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmyencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msd2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1481\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1482\u001b[1;33m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[0;32m   1483\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0;32m   1484\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Encoder:\n\tUnexpected key(s) in state_dict: \"resnet.0.weight\", \"resnet.1.weight\", \"resnet.1.bias\", \"resnet.1.running_mean\", \"resnet.1.running_var\", \"resnet.4.0.conv1.weight\", \"resnet.4.0.bn1.weight\", \"resnet.4.0.bn1.bias\", \"resnet.4.0.bn1.running_mean\", \"resnet.4.0.bn1.running_var\", \"resnet.4.0.conv2.weight\", \"resnet.4.0.bn2.weight\", \"resnet.4.0.bn2.bias\", \"resnet.4.0.bn2.running_mean\", \"resnet.4.0.bn2.running_var\", \"resnet.4.0.conv3.weight\", \"resnet.4.0.bn3.weight\", \"resnet.4.0.bn3.bias\", \"resnet.4.0.bn3.running_mean\", \"resnet.4.0.bn3.running_var\", \"resnet.4.0.downsample.0.weight\", \"resnet.4.0.downsample.1.weight\", \"resnet.4.0.downsample.1.bias\", \"resnet.4.0.downsample.1.running_mean\", \"resnet.4.0.downsample.1.running_var\", \"resnet.4.1.conv1.weight\", \"resnet.4.1.bn1.weight\", \"resnet.4.1.bn1.bias\", \"resnet.4.1.bn1.running_mean\", \"resnet.4.1.bn1.running_var\", \"resnet.4.1.conv2.weight\", \"resnet.4.1.bn2.weight\", \"resnet.4.1.bn2.bias\", \"resnet.4.1.bn2.running_mean\", \"resnet.4.1.bn2.running_var\", \"resnet.4.1.conv3.weight\", \"resnet.4.1.bn3.weight\", \"resnet.4.1.bn3.bias\", \"resnet.4.1.bn3.running_mean\", \"resnet.4.1.bn3.running_var\", \"resnet.4.2.conv1.weight\", \"resnet.4.2.bn1.weight\", \"resnet.4.2.bn1.bias\", \"resnet.4.2.bn1.running_mean\", \"resnet.4.2.bn1.running_var\", \"resnet.4.2.conv2.weight\", \"resnet.4.2.bn2.weight\", \"resnet.4.2.bn2.bias\", \"resnet.4.2.bn2.running_mean\", \"resnet.4.2.bn2.running_var\", \"resnet.4.2.conv3.weight\", \"resnet.4.2.bn3.weight\", \"resnet.4.2.bn3.bias\", \"resnet.4.2.bn3.running_mean\", \"resnet.4.2.bn3.running_var\", \"resnet.5.0.conv1.weight\", \"resnet.5.0.bn1.weight\", \"resnet.5.0.bn1.bias\", \"resnet.5.0.bn1.running_mean\", \"resnet.5.0.bn1.running_var\", \"resnet.5.0.conv2.weight\", \"resnet.5.0.bn2.weight\", \"resnet.5.0.bn2.bias\", \"resnet.5.0.bn2.running_mean\", \"resnet.5.0.bn2.running_var\", \"resnet.5.0.conv3.weight\", \"resnet.5.0.bn3.weight\", \"resnet.5.0.bn3.bias\", \"resnet.5.0.bn3.running_mean\", \"resnet.5.0.bn3.running_var\", \"resnet.5.0.downsample.0.weight\", \"resnet.5.0.downsample.1.weight\", \"resnet.5.0.downsample.1.bias\", \"resnet.5.0.downsample.1.running_mean\", \"resnet.5.0.downsample.1.running_var\", \"resnet.5.1.conv1.weight\", \"resnet.5.1.bn1.weight\", \"resnet.5.1.bn1.bias\", \"resnet.5.1.bn1.running_mean\", \"resnet.5.1.bn1.running_var\", \"resnet.5.1.conv2.weight\", \"resnet.5.1.bn2.weight\", \"resnet.5.1.bn2.bias\", \"resnet.5.1.bn2.running_mean\", \"resnet.5.1.bn2.running_var\", \"resnet.5.1.conv3.weight\", \"resnet.5.1.bn3.weight\", \"resnet.5.1.bn3.bias\", \"resnet.5.1.bn3.running_mean\", \"resnet.5.1.bn3.running_var\", \"resnet.5.2.conv1.weight\", \"resnet.5.2.bn1.weight\", \"resnet.5.2.bn1.bias\", \"resnet.5.2.bn1.running_mean\", \"resnet.5.2.bn1.running_var\", \"resnet.5.2.conv2.weight\", \"resnet.5.2.bn2.weight\", \"resnet.5.2.bn2.bias\", \"resnet.5.2.bn2.running_mean\", \"resnet.5.2.bn2.running_var\", \"resnet.5.2.conv3.weight\", \"resnet.5.2.bn3.weight\", \"resnet.5.2.bn3.bias\", \"resnet.5.2.bn3.running_mean\", \"resnet.5.2.bn3.running_var\", \"resnet.5.3.conv1.weight\", \"resnet.5.3.bn1.weight\", \"resnet.5.3.bn1.bias\", \"resnet.5.3.bn1.running_mean\", \"resnet.5.3.bn1.running_var\", \"resnet.5.3.conv2.weight\", \"resnet.5.3.bn2.weight\", \"resnet.5.3.bn2.bias\", \"resnet.5.3.bn2.running_mean\", \"resnet.5.3.bn2.running_var\", \"resnet.5.3.conv3.weight\", \"resnet.5.3.bn3.weight\", \"resnet.5.3.bn3.bias\", \"resnet.5.3.bn3.running_mean\", \"resnet.5.3.bn3.running_var\", \"resnet.6.3.conv1.weight\", \"resnet.6.3.bn1.weight\", \"resnet.6.3.bn1.bias\", \"resnet.6.3.bn1.running_mean\", \"resnet.6.3.bn1.running_var\", \"resnet.6.3.conv2.weight\", \"resnet.6.3.bn2.weight\", \"resnet.6.3.bn2.bias\", \"resnet.6.3.bn2.running_mean\", \"resnet.6.3.bn2.running_var\", \"resnet.6.3.conv3.weight\", \"resnet.6.3.bn3.weight\", \"resnet.6.3.bn3.bias\", \"resnet.6.3.bn3.running_mean\", \"resnet.6.3.bn3.running_var\", \"resnet.6.4.conv1.weight\", \"resnet.6.4.bn1.weight\", \"resnet.6.4.bn1.bias\", \"resnet.6.4.bn1.running_mean\", \"resnet.6.4.bn1.running_var\", \"resnet.6.4.conv2.weight\", \"resnet.6.4.bn2.weight\", \"resnet.6.4.bn2.bias\", \"resnet.6.4.bn2.running_mean\", \"resnet.6.4.bn2.running_var\", \"resnet.6.4.conv3.weight\", \"resnet.6.4.bn3.weight\", \"resnet.6.4.bn3.bias\", \"resnet.6.4.bn3.running_mean\", \"resnet.6.4.bn3.running_var\", \"resnet.6.5.conv1.weight\", \"resnet.6.5.bn1.weight\", \"resnet.6.5.bn1.bias\", \"resnet.6.5.bn1.running_mean\", \"resnet.6.5.bn1.running_var\", \"resnet.6.5.conv2.weight\", \"resnet.6.5.bn2.weight\", \"resnet.6.5.bn2.bias\", \"resnet.6.5.bn2.running_mean\", \"resnet.6.5.bn2.running_var\", \"resnet.6.5.conv3.weight\", \"resnet.6.5.bn3.weight\", \"resnet.6.5.bn3.bias\", \"resnet.6.5.bn3.running_mean\", \"resnet.6.5.bn3.running_var\", \"resnet.6.6.conv1.weight\", \"resnet.6.6.bn1.weight\", \"resnet.6.6.bn1.bias\", \"resnet.6.6.bn1.running_mean\", \"resnet.6.6.bn1.running_var\", \"resnet.6.6.conv2.weight\", \"resnet.6.6.bn2.weight\", \"resnet.6.6.bn2.bias\", \"resnet.6.6.bn2.running_mean\", \"resnet.6.6.bn2.running_var\", \"resnet.6.6.conv3.weight\", \"resnet.6.6.bn3.weight\", \"resnet.6.6.bn3.bias\", \"resnet.6.6.bn3.running_mean\", \"resnet.6.6.bn3.running_var\", \"resnet.6.7.conv1.weight\", \"resnet.6.7.bn1.weight\", \"resnet.6.7.bn1.bias\", \"resnet.6.7.bn1.running_mean\", \"resnet.6.7.bn1.running_var\", \"resnet.6.7.conv2.weight\", \"resnet.6.7.bn2.weight\", \"resnet.6.7.bn2.bias\", \"resnet.6.7.bn2.running_mean\", \"resnet.6.7.bn2.running_var\", \"resnet.6.7.conv3.weight\", \"resnet.6.7.bn3.weight\", \"resnet.6.7.bn3.bias\", \"resnet.6.7.bn3.running_mean\", \"resnet.6.7.bn3.running_var\", \"resnet.6.8.conv1.weight\", \"resnet.6.8.bn1.weight\", \"resnet.6.8.bn1.bias\", \"resnet.6.8.bn1.running_mean\", \"resnet.6.8.bn1.running_var\", \"resnet.6.8.conv2.weight\", \"resnet.6.8.bn2.weight\", \"resnet.6.8.bn2.bias\", \"resnet.6.8.bn2.running_mean\", \"resnet.6.8.bn2.running_var\", \"resnet.6.8.conv3.weight\", \"resnet.6.8.bn3.weight\", \"resnet.6.8.bn3.bias\", \"resnet.6.8.bn3.running_mean\", \"resnet.6.8.bn3.running_var\", \"resnet.6.9.conv1.weight\", \"resnet.6.9.bn1.weight\", \"resnet.6.9.bn1.bias\", \"resnet.6.9.bn1.running_mean\", \"resnet.6.9.bn1.running_var\", \"resnet.6.9.conv2.weight\", \"resnet.6.9.bn2.weight\", \"resnet.6.9.bn2.bias\", \"resnet.6.9.bn2.running_mean\", \"resnet.6.9.bn2.running_var\", \"resnet.6.9.conv3.weight\", \"resnet.6.9.bn3.weight\", \"resnet.6.9.bn3.bias\", \"resnet.6.9.bn3.running_mean\", \"resnet.6.9.bn3.running_var\", \"resnet.6.10.conv1.weight\", \"resnet.6.10.bn1.weight\", \"resnet.6.10.bn1.bias\", \"resnet.6.10.bn1.running_mean\", \"resnet.6.10.bn1.running_var\", \"resnet.6.10.conv2.weight\", \"resnet.6.10.bn2.weight\", \"resnet.6.10.bn2.bias\", \"resnet.6.10.bn2.running_mean\", \"resnet.6.10.bn2.running_var\", \"resnet.6.10.conv3.weight\", \"resnet.6.10.bn3.weight\", \"resnet.6.10.bn3.bias\", \"resnet.6.10.bn3.running_mean\", \"resnet.6.10.bn3.running_var\", \"resnet.6.11.conv1.weight\", \"resnet.6.11.bn1.weight\", \"resnet.6.11.bn1.bias\", \"resnet.6.11.bn1.running_mean\", \"resnet.6.11.bn1.running_var\", \"resnet.6.11.conv2.weight\", \"resnet.6.11.bn2.weight\", \"resnet.6.11.bn2.bias\", \"resnet.6.11.bn2.running_mean\", \"resnet.6.11.bn2.running_var\", \"resnet.6.11.conv3.weight\", \"resnet.6.11.bn3.weight\", \"resnet.6.11.bn3.bias\", \"resnet.6.11.bn3.running_mean\", \"resnet.6.11.bn3.running_var\", \"resnet.6.12.conv1.weight\", \"resnet.6.12.bn1.weight\", \"resnet.6.12.bn1.bias\", \"resnet.6.12.bn1.running_mean\", \"resnet.6.12.bn1.running_var\", \"resnet.6.12.conv2.weight\", \"resnet.6.12.bn2.weight\", \"resnet.6.12.bn2.bias\", \"resnet.6.12.bn2.running_mean\", \"resnet.6.12.bn2.running_var\", \"resnet.6.12.conv3.weight\", \"resnet.6.12.bn3.weight\", \"resnet.6.12.bn3.bias\", \"resnet.6.12.bn3.running_mean\", \"resnet.6.12.bn3.running_var\", \"resnet.6.13.conv1.weight\", \"resnet.6.13.bn1.weight\", \"resnet.6.13.bn1.bias\", \"resnet.6.13.bn1.running_mean\", \"resnet.6.13.bn1.running_var\", \"resnet.6.13.conv2.weight\", \"resnet.6.13.bn2.weight\", \"resnet.6.13.bn2.bias\", \"resnet.6.13.bn2.running_mean\", \"resnet.6.13.bn2.running_var\", \"resnet.6.13.conv3.weight\", \"resnet.6.13.bn3.weight\", \"resnet.6.13.bn3.bias\", \"resnet.6.13.bn3.running_mean\", \"resnet.6.13.bn3.running_var\", \"resnet.6.14.conv1.weight\", \"resnet.6.14.bn1.weight\", \"resnet.6.14.bn1.bias\", \"resnet.6.14.bn1.running_mean\", \"resnet.6.14.bn1.running_var\", \"resnet.6.14.conv2.weight\", \"resnet.6.14.bn2.weight\", \"resnet.6.14.bn2.bias\", \"resnet.6.14.bn2.running_mean\", \"resnet.6.14.bn2.running_var\", \"resnet.6.14.conv3.weight\", \"resnet.6.14.bn3.weight\", \"resnet.6.14.bn3.bias\", \"resnet.6.14.bn3.running_mean\", \"resnet.6.14.bn3.running_var\", \"resnet.6.15.conv1.weight\", \"resnet.6.15.bn1.weight\", \"resnet.6.15.bn1.bias\", \"resnet.6.15.bn1.running_mean\", \"resnet.6.15.bn1.running_var\", \"resnet.6.15.conv2.weight\", \"resnet.6.15.bn2.weight\", \"resnet.6.15.bn2.bias\", \"resnet.6.15.bn2.running_mean\", \"resnet.6.15.bn2.running_var\", \"resnet.6.15.conv3.weight\", \"resnet.6.15.bn3.weight\", \"resnet.6.15.bn3.bias\", \"resnet.6.15.bn3.running_mean\", \"resnet.6.15.bn3.running_var\", \"resnet.6.16.conv1.weight\", \"resnet.6.16.bn1.weight\", \"resnet.6.16.bn1.bias\", \"resnet.6.16.bn1.running_mean\", \"resnet.6.16.bn1.running_var\", \"resnet.6.16.conv2.weight\", \"resnet.6.16.bn2.weight\", \"resnet.6.16.bn2.bias\", \"resnet.6.16.bn2.running_mean\", \"resnet.6.16.bn2.running_var\", \"resnet.6.16.conv3.weight\", \"resnet.6.16.bn3.weight\", \"resnet.6.16.bn3.bias\", \"resnet.6.16.bn3.running_mean\", \"resnet.6.16.bn3.running_var\", \"resnet.6.17.conv1.weight\", \"resnet.6.17.bn1.weight\", \"resnet.6.17.bn1.bias\", \"resnet.6.17.bn1.running_mean\", \"resnet.6.17.bn1.running_var\", \"resnet.6.17.conv2.weight\", \"resnet.6.17.bn2.weight\", \"resnet.6.17.bn2.bias\", \"resnet.6.17.bn2.running_mean\", \"resnet.6.17.bn2.running_var\", \"resnet.6.17.conv3.weight\", \"resnet.6.17.bn3.weight\", \"resnet.6.17.bn3.bias\", \"resnet.6.17.bn3.running_mean\", \"resnet.6.17.bn3.running_var\", \"resnet.6.18.conv1.weight\", \"resnet.6.18.bn1.weight\", \"resnet.6.18.bn1.bias\", \"resnet.6.18.bn1.running_mean\", \"resnet.6.18.bn1.running_var\", \"resnet.6.18.conv2.weight\", \"resnet.6.18.bn2.weight\", \"resnet.6.18.bn2.bias\", \"resnet.6.18.bn2.running_mean\", \"resnet.6.18.bn2.running_var\", \"resnet.6.18.conv3.weight\", \"resnet.6.18.bn3.weight\", \"resnet.6.18.bn3.bias\", \"resnet.6.18.bn3.running_mean\", \"resnet.6.18.bn3.running_var\", \"resnet.6.19.conv1.weight\", \"resnet.6.19.bn1.weight\", \"resnet.6.19.bn1.bias\", \"resnet.6.19.bn1.running_mean\", \"resnet.6.19.bn1.running_var\", \"resnet.6.19.conv2.weight\", \"resnet.6.19.bn2.weight\", \"resnet.6.19.bn2.bias\", \"resnet.6.19.bn2.running_mean\", \"resnet.6.19.bn2.running_var\", \"resnet.6.19.conv3.weight\", \"resnet.6.19.bn3.weight\", \"resnet.6.19.bn3.bias\", \"resnet.6.19.bn3.running_mean\", \"resnet.6.19.bn3.running_var\", \"resnet.6.20.conv1.weight\", \"resnet.6.20.bn1.weight\", \"resnet.6.20.bn1.bias\", \"resnet.6.20.bn1.running_mean\", \"resnet.6.20.bn1.running_var\", \"resnet.6.20.conv2.weight\", \"resnet.6.20.bn2.weight\", \"resnet.6.20.bn2.bias\", \"resnet.6.20.bn2.running_mean\", \"resnet.6.20.bn2.running_var\", \"resnet.6.20.conv3.weight\", \"resnet.6.20.bn3.weight\", \"resnet.6.20.bn3.bias\", \"resnet.6.20.bn3.running_mean\", \"resnet.6.20.bn3.running_var\", \"resnet.6.21.conv1.weight\", \"resnet.6.21.bn1.weight\", \"resnet.6.21.bn1.bias\", \"resnet.6.21.bn1.running_mean\", \"resnet.6.21.bn1.running_var\", \"resnet.6.21.conv2.weight\", \"resnet.6.21.bn2.weight\", \"resnet.6.21.bn2.bias\", \"resnet.6.21.bn2.running_mean\", \"resnet.6.21.bn2.running_var\", \"resnet.6.21.conv3.weight\", \"resnet.6.21.bn3.weight\", \"resnet.6.21.bn3.bias\", \"resnet.6.21.bn3.running_mean\", \"resnet.6.21.bn3.running_var\", \"resnet.6.22.conv1.weight\", \"resnet.6.22.bn1.weight\", \"resnet.6.22.bn1.bias\", \"resnet.6.22.bn1.running_mean\", \"resnet.6.22.bn1.running_var\", \"resnet.6.22.conv2.weight\", \"resnet.6.22.bn2.weight\", \"resnet.6.22.bn2.bias\", \"resnet.6.22.bn2.running_mean\", \"resnet.6.22.bn2.running_var\", \"resnet.6.22.conv3.weight\", \"resnet.6.22.bn3.weight\", \"resnet.6.22.bn3.bias\", \"resnet.6.22.bn3.running_mean\", \"resnet.6.22.bn3.running_var\". \n\tsize mismatch for resnet.6.0.conv1.weight: copying a param with shape torch.Size([256, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n\tsize mismatch for resnet.6.0.bn1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.0.bn1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.0.bn1.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.0.bn1.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.0.conv2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for resnet.6.0.bn2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.0.bn2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.0.bn2.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.0.bn2.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.0.conv3.weight: copying a param with shape torch.Size([1024, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 64, 1, 1]).\n\tsize mismatch for resnet.6.0.bn3.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.6.0.bn3.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.6.0.bn3.running_mean: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.6.0.bn3.running_var: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.6.0.downsample.0.weight: copying a param with shape torch.Size([1024, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 64, 1, 1]).\n\tsize mismatch for resnet.6.0.downsample.1.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.6.0.downsample.1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.6.0.downsample.1.running_mean: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.6.0.downsample.1.running_var: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.6.1.conv1.weight: copying a param with shape torch.Size([256, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n\tsize mismatch for resnet.6.1.bn1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.1.bn1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.1.bn1.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.1.bn1.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.1.conv2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for resnet.6.1.bn2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.1.bn2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.1.bn2.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.1.bn2.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.1.conv3.weight: copying a param with shape torch.Size([1024, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 64, 1, 1]).\n\tsize mismatch for resnet.6.1.bn3.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.6.1.bn3.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.6.1.bn3.running_mean: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.6.1.bn3.running_var: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.6.2.conv1.weight: copying a param with shape torch.Size([256, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n\tsize mismatch for resnet.6.2.bn1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.2.bn1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.2.bn1.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.2.bn1.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.2.conv2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for resnet.6.2.bn2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.2.bn2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.2.bn2.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.2.bn2.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for resnet.6.2.conv3.weight: copying a param with shape torch.Size([1024, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 64, 1, 1]).\n\tsize mismatch for resnet.6.2.bn3.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.6.2.bn3.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.6.2.bn3.running_mean: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.6.2.bn3.running_var: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for resnet.7.0.conv1.weight: copying a param with shape torch.Size([512, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for resnet.7.0.bn1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.0.bn1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.0.bn1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.0.bn1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.0.conv2.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for resnet.7.0.bn2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.0.bn2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.0.bn2.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.0.bn2.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.0.conv3.weight: copying a param with shape torch.Size([2048, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 128, 1, 1]).\n\tsize mismatch for resnet.7.0.bn3.weight: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.7.0.bn3.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.7.0.bn3.running_mean: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.7.0.bn3.running_var: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.7.0.downsample.0.weight: copying a param with shape torch.Size([2048, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for resnet.7.0.downsample.1.weight: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.7.0.downsample.1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.7.0.downsample.1.running_mean: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.7.0.downsample.1.running_var: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.7.1.conv1.weight: copying a param with shape torch.Size([512, 2048, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for resnet.7.1.bn1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.1.bn1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.1.bn1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.1.bn1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.1.conv2.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for resnet.7.1.bn2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.1.bn2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.1.bn2.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.1.bn2.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.1.conv3.weight: copying a param with shape torch.Size([2048, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 128, 1, 1]).\n\tsize mismatch for resnet.7.1.bn3.weight: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.7.1.bn3.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.7.1.bn3.running_mean: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.7.1.bn3.running_var: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.7.2.conv1.weight: copying a param with shape torch.Size([512, 2048, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for resnet.7.2.bn1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.2.bn1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.2.bn1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.2.bn1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.2.conv2.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for resnet.7.2.bn2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.2.bn2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.2.bn2.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.2.bn2.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for resnet.7.2.conv3.weight: copying a param with shape torch.Size([2048, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 128, 1, 1]).\n\tsize mismatch for resnet.7.2.bn3.weight: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.7.2.bn3.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.7.2.bn3.running_mean: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.7.2.bn3.running_var: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512])."
     ]
    }
   ],
   "source": [
    "myencoder.load_state_dict(sd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Bottleneck' object has no attribute 'dequant'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17324/1465046204.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17324/854174693.py\u001b[0m in \u001b[0;36minference\u001b[1;34m(encoder, decoder, img, word_map, beam_size)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# INFERENCE FUNCTION\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeam_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mseq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malphas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcaption\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaption_image_beam_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0malphas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malphas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mrev_word_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\TUM\\WS 2021-2022\\Advanced Topics in Communication Electronics\\image-captioning-on-pytorch-master\\image-captioning-on-pytorch-master\\caption.py\u001b[0m in \u001b[0;36mcaption_image_beam_search\u001b[1;34m(encoder, decoder, image_path, word_map, beam_size)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;31m# Encode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (1, 3, 256, 256)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0mencoder_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (1, enc_image_size, enc_image_size, encoder_dim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m     \u001b[0menc_image_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mencoder_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\TUM\\WS 2021-2022\\Advanced Topics in Communication Electronics\\image-captioning-on-pytorch-master\\image-captioning-on-pytorch-master\\models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, images)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mencoded\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \"\"\"\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (batch_size, 2048, image_size/32, image_size/32)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madaptive_pool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (batch_size, 2048, encoded_image_size, encoded_image_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (batch_size, encoded_image_size, encoded_image_size, 2048)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m             \u001b[0midentity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdequant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0midentity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1175\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[0;32m   1178\u001b[0m             type(self).__name__, name))\n\u001b[0;32m   1179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Bottleneck' object has no attribute 'dequant'"
     ]
    }
   ],
   "source": [
    "inference(encoder, decoder, img, word_map, beam_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(myencoder, decoder, img, word_map, beam_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_my_encoder(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(myencoder, decoder, img, word_map, beam_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_encoder_sparsity = 0\n",
    "den = 0\n",
    "num = 0\n",
    "for name, module in myencoder.named_modules():\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        print(\"Sparsity in {} (shape = {}): {:.2f}%\".format\n",
    "        (name, module.weight.shape,\n",
    "        100. * float(torch.sum(module.weight == 0))/ float(module.weight.nelement())\n",
    "        ))\n",
    "        num += float(torch.sum(module.weight == 0))\n",
    "        den += float(module.weight.nelement())\n",
    "    elif isinstance(module, torch.nn.Linear):\n",
    "        print(\"Sparsity in {} (shape = {}): {:.2f}%\".format\n",
    "        (name, module.weight.shape,\n",
    "        100. * float(torch.sum(module.weight == 0))/ float(module.weight.nelement())\n",
    "        ))\n",
    "        num += float(torch.sum(module.weight == 0))\n",
    "        den += float(module.weight.nelement())\n",
    "total_encoder_sparsity = 100.*num/den        \n",
    "print(\"Total sparsity in the pruned encoder: {:.2f}%\".format(total_encoder_sparsity))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in myencoder.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear) or isinstance(module, torch.nn.LSTM):\n",
    "        quantized_model = quantization.quantize_dynamic(module, {torch.nn.LSTM, torch.nn.Linear}, dtype=torch.qint8)\n",
    "        for n, p in quantized_model.named_parameters():\n",
    "            print(n, 'has type:', p.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUANTIZATION FUNCTION\n",
    "def quantize_my_model(model):\n",
    "    # set the qconfig for PTQ\n",
    "    model.qconfig = quantization.get_default_qconfig('fbgemm')\n",
    "    # set the qengine to control weight packing\n",
    "    torch.backends.quantized.engine = 'fbgemm'\n",
    "    # put model in eval mode\n",
    "    model.eval()\n",
    "    quantization.prepare(model, inplace=True)\n",
    "    quantization.convert(myencoder, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize_my_model(myencoder)\n",
    "print(myencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inference(myencoder, decoder, img, word_map, beam_size)\n",
    "# model.to('QuantizedCPU')\n",
    "# summary(myencoder, (3,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify quantization configuration\n",
    "# # Start with simple min/max range estimation and per-tensor\n",
    "# # quantization of weights\n",
    "# myencoder.qconfig = quantization.default_qconfig\n",
    "# # Convert to quantized model\n",
    "# quantization.convert(myencoder, inplace=True)\n",
    "# # Calibrate with the training set\n",
    "# # evaluate(myModel, criterion, data_loader, neval_batches=num_calibration_batches)\n",
    "# for name, module in myencoder.named_modules(): \n",
    "#     for n, p in module.named_parameters():\n",
    "#             print(n, 'has type:', p.dtype)\n",
    "# # prunedencoder = Encoder()\n",
    "# # myencoder_state_dict = myencoder.state_dict()\n",
    "# # prunedencoder_state_dict = prunedencoder.state_dict()\n",
    "# # for key in myencoder_state_dict.keys():\n",
    "# #     print(key)\n",
    "#     #     if key in prunedencoder_state_dict.keys():\n",
    "# #         prunedencoder_state_dict[key] = myencoder_state_dict[key]\n",
    "# # prunedencoder.load_state_dict(myencoder_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# summary(myencoder, (3,256,256))\n",
    "for name, param in myencoder.named_parameters():\n",
    "    print(param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(encoder, (3,256,256))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "224556cf7c5c9b055ae49a6675c30d5f2054c9ee267b704439a1be7caf4d323c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
